'''Unit -03 - 02 Write a code to minimize the cost function (mean squared error) in the linear regression
using gradient descent (an iterative optimization algorithm, which finds the minimum of a
differentiable function) with at least two independent variables. Determine the correlation
matrix for the regression parameters. '''
import numpy as np
import matplotlib.pyplot as plt

# Generate some sample data (2 independent variables)
np.random.seed(42)
m = 100  # number of samples
X1 = 2 * np.random.rand(m)
X2 = 3 * np.random.rand(m)
noise = np.random.randn(m) * 0.5
y = 4 + 3 * X1 + 5 * X2 + noise  # true model: y = 4 + 3*X1 + 5*X2 + noise

# Prepare feature matrix (adding bias term)
X = np.c_[np.ones((m, 1)), X1, X2]  # shape (m, 3)

# Initialize parameters
theta = np.random.randn(3, 1)  # theta0, theta1, theta2
learning_rate = 0.1
n_iterations = 500
loss_history = []

# Gradient Descent
for iteration in range(n_iterations):
    gradients = (2/m) * X.T @ (X @ theta - y.reshape(-1,1))
    theta -= learning_rate * gradients
    loss = (1/m) * np.sum((X @ theta - y.reshape(-1,1))**2)
    loss_history.append(loss)

# Final parameter values
theta0, theta1, theta2 = theta.flatten()
print(f"Theta 0 (Intercept): {theta0:.4f}")
print(f"Theta 1 (Slope for X1): {theta1:.4f}")
print(f"Theta 2 (Slope for X2): {theta2:.4f}")

# Calculate correlation matrix of parameters
# Covariance matrix of estimated theta
residuals = y.reshape(-1,1) - X @ theta
sigma_squared = (residuals.T @ residuals) / (m - 3)
cov_matrix = sigma_squared[0,0] * np.linalg.inv(X.T @ X)

# Correlation matrix
std_devs = np.sqrt(np.diag(cov_matrix))
correlation_matrix = cov_matrix / (std_devs[:, None] * std_devs[None, :])

print("\nCorrelation matrix of parameters:")
print(np.round(correlation_matrix, 4))

# Plotting Loss vs Iterations
plt.figure(figsize=(8, 5))
plt.plot(range(n_iterations), loss_history, color="blue")
plt.xlabel("Iterations")
plt.ylabel("Mean Squared Error (Loss) = Cost Function")
plt.title("Cost Function vs Iterations ")
plt.grid(True)
plt.show()
